{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_loader import LibriSpeechDataLoader\n",
    "from utils.feature_extractor import FeatureExtractor\n",
    "import config\n",
    "\n",
    "# Load character mappings\n",
    "char_to_num = np.load('char_to_num.npy', allow_pickle=True).item()\n",
    "num_to_char = np.load('num_to_char.npy', allow_pickle=True).item()\n",
    "\n",
    "print(f\"Loaded character mapping with {len(char_to_num)} characters\")\n",
    "\n",
    "# Initialize data loader and feature extractor\n",
    "data_loader = LibriSpeechDataLoader(config.DATASET_CONFIG)\n",
    "data_loader.char_to_num = char_to_num\n",
    "data_loader.num_to_char = num_to_char\n",
    "\n",
    "feature_extractor = FeatureExtractor(\n",
    "    sampling_rate=config.DATASET_CONFIG['sampling_rate'],\n",
    "    n_mfcc=config.FEATURE_CONFIG['n_mfcc'],\n",
    "    n_mels=config.FEATURE_CONFIG['n_mels'],\n",
    "    n_fft=config.FEATURE_CONFIG['n_fft'],\n",
    "    hop_length=config.FEATURE_CONFIG['hop_length']\n",
    ")\n",
    "\n",
    "# Load a sample for feature visualization\n",
    "print(\"Loading sample data for feature visualization...\")\n",
    "datasets = data_loader.load_dataset(splits=['train'])\n",
    "\n",
    "for example in datasets['train'].take(1):\n",
    "    sample_audio = example['audio'].numpy()\n",
    "    sample_text = example['text'].numpy().decode('utf-8')\n",
    "    break\n",
    "\n",
    "print(f\"Sample audio shape: {sample_audio.shape}\")\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "\n",
    "# Extract and visualize all feature types\n",
    "print(\"\\nExtracting and visualizing features...\")\n",
    "features = feature_extractor.visualize_features(\n",
    "    sample_audio, \n",
    "    title=f\"Feature Extraction - '{sample_text}'\"\n",
    ")\n",
    "\n",
    "# Print feature shapes\n",
    "for name, feature in features.items():\n",
    "    print(f\"{name}: {feature.shape}\")\n",
    "\n",
    "# Function to create dataset with specific feature type\n",
    "def create_feature_dataset(dataset, feature_type='mfcc'):\n",
    "    \"\"\"Create dataset with specific feature type\"\"\"\n",
    "    def process_example(example):\n",
    "        audio = example['audio']\n",
    "        text = example['text']\n",
    "        \n",
    "        # Extract features based on type\n",
    "        if feature_type == 'mfcc':\n",
    "            features = tf.py_function(\n",
    "                func=lambda a: feature_extractor.extract_mfcc(a),\n",
    "                inp=[audio],\n",
    "                Tout=tf.float32\n",
    "            )\n",
    "        elif feature_type == 'mel_spectrogram':\n",
    "            features = tf.py_function(\n",
    "                func=lambda a: feature_extractor.extract_mel_spectrogram(a),\n",
    "                inp=[audio],\n",
    "                Tout=tf.float32\n",
    "            )\n",
    "        elif feature_type == 'wavelet':\n",
    "            features = tf.py_function(\n",
    "                func=lambda a: feature_extractor.extract_wavelet_time_frequency(a),\n",
    "                inp=[audio],\n",
    "                Tout=tf.float32\n",
    "            )\n",
    "        elif feature_type == 'combined':\n",
    "            features = tf.py_function(\n",
    "                func=lambda a: feature_extractor.extract_combined_features(a),\n",
    "                inp=[audio],\n",
    "                Tout=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature type: {feature_type}\")\n",
    "        \n",
    "        # Convert text to numbers\n",
    "        text_numbers = tf.py_function(\n",
    "            func=lambda t: data_loader.text_to_numbers(t.numpy().decode('utf-8')),\n",
    "            inp=[text],\n",
    "            Tout=tf.int32\n",
    "        )\n",
    "        \n",
    "        return features, text_numbers\n",
    "    \n",
    "    processed_ds = dataset.map(\n",
    "        process_example,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Pad sequences\n",
    "    def pad_data(x, y):\n",
    "        x_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            x, padding='post', dtype='float32'\n",
    "        )\n",
    "        y_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [y], padding='post', dtype='int32'\n",
    "        )[0]\n",
    "        return x_padded, y_padded\n",
    "    \n",
    "    padded_ds = processed_ds.map(\n",
    "        lambda x, y: tf.py_function(\n",
    "            func=lambda a, b: pad_data(a.numpy(), b.numpy()),\n",
    "            inp=[x, y],\n",
    "            Tout=[tf.float32, tf.int32]\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    return padded_ds.batch(config.DATASET_CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create datasets for different feature types\n",
    "print(\"\\nCreating datasets for different feature types...\")\n",
    "feature_datasets = {}\n",
    "\n",
    "for feature_type in config.FEATURE_CONFIG['feature_types']:\n",
    "    print(f\"Creating {feature_type} dataset...\")\n",
    "    \n",
    "    train_ds = create_feature_dataset(datasets['train'], feature_type)\n",
    "    val_ds = create_feature_dataset(datasets['validation'], feature_type)\n",
    "    test_ds = create_feature_dataset(datasets['test'], feature_type)\n",
    "    \n",
    "    feature_datasets[feature_type] = {\n",
    "        'train': train_ds,\n",
    "        'val': val_ds,\n",
    "        'test': test_ds\n",
    "    }\n",
    "    \n",
    "    # Test the dataset\n",
    "    for features, labels in train_ds.take(1):\n",
    "        print(f\"{feature_type} - Features shape: {features.shape}, Labels shape: {labels.shape}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nFeature extraction completed!\")\n",
    "print(\"Available feature datasets:\", list(feature_datasets.keys()))\n",
    "\n",
    "# Save feature datasets info\n",
    "feature_info = {}\n",
    "for feature_type, datasets in feature_datasets.items():\n",
    "    for features, labels in datasets['train'].take(1):\n",
    "        feature_info[feature_type] = {\n",
    "            'input_dim': features.shape[2],\n",
    "            'output_dim': len(char_to_num)\n",
    "        }\n",
    "\n",
    "np.save('feature_info.npy', feature_info)\n",
    "print(\"Feature information saved!\")\n",
    "\n",
    "print(\"\\nNext: Train models using different feature types\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
