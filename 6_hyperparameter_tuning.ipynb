{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from utils.model_utils import create_cnn_model, create_lstm_model, create_transformer_model\n",
    "from utils.data_loader import LibriSpeechDataLoader\n",
    "from utils.evaluation import ModelEvaluator\n",
    "import config\n",
    "\n",
    "# Set random seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load mappings and feature info\n",
    "char_to_num = np.load('char_to_num.npy', allow_pickle=True).item()\n",
    "num_to_char = np.load('num_to_char.npy', allow_pickle=True).item()\n",
    "feature_info = np.load('feature_info.npy', allow_pickle=True).item()\n",
    "\n",
    "print(\"Hyperparameter Tuning\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Choose the best feature type based on previous results\n",
    "best_feature_type = 'mfcc'  # You can change this based on your results\n",
    "print(f\"Using {best_feature_type} features for hyperparameter tuning\")\n",
    "\n",
    "# Load datasets\n",
    "feature_datasets = np.load(f'feature_datasets_{best_feature_type}.npy', allow_pickle=True).item()\n",
    "train_ds = feature_datasets['train']\n",
    "val_ds = feature_datasets['val']\n",
    "\n",
    "# Get model dimensions\n",
    "input_dim = feature_info[best_feature_type]['input_dim']\n",
    "output_dim = len(char_to_char)\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Output dimension: {output_dim}\")\n",
    "\n",
    "# Prepare datasets for CTC training\n",
    "def prepare_ctc_data(dataset):\n",
    "    def add_ctc_inputs(features, labels):\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        input_length = tf.ones((batch_size, 1)) * tf.shape(features)[1]\n",
    "        label_length = tf.ones((batch_size, 1)) * tf.shape(labels)[1]\n",
    "        dummy_output = tf.zeros(batch_size)\n",
    "        \n",
    "        return {\n",
    "            'input': features,\n",
    "            'y_true': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length\n",
    "        }, dummy_output\n",
    "    \n",
    "    return dataset.map(add_ctc_inputs)\n",
    "\n",
    "train_ctc_ds = prepare_ctc_data(train_ds)\n",
    "val_ctc_ds = prepare_ctc_data(val_ds)\n",
    "\n",
    "# CNN Hyperparameter Tuning\n",
    "def build_cnn_model(hp):\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    filters = hp.Int('filters', min_value=32, max_value=128, step=32)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=64)\n",
    "    \n",
    "    model = create_cnn_model(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        filters=filters,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# LSTM Hyperparameter Tuning\n",
    "def build_lstm_model(hp):\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=64)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    num_layers = hp.Int('num_layers', min_value=2, max_value=4)\n",
    "    \n",
    "    model = create_lstm_model(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        lstm_units=lstm_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Transformer Hyperparameter Tuning\n",
    "def build_transformer_model(hp):\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    d_model = hp.Int('d_model', min_value=64, max_value=256, step=64)\n",
    "    num_heads = hp.Choice('num_heads', values=[4, 8, 16])\n",
    "    ff_dim = hp.Int('ff_dim', min_value=256, max_value=1024, step=256)\n",
    "    num_layers = hp.Int('num_layers', min_value=2, max_value=6)\n",
    "    \n",
    "    model = create_transformer_model(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize tuners\n",
    "print(\"\\nInitializing Hyperparameter Tuners...\")\n",
    "\n",
    "cnn_tuner = kt.Hyperband(\n",
    "    build_cnn_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory='tuning',\n",
    "    project_name=f'cnn_{best_feature_type}'\n",
    ")\n",
    "\n",
    "lstm_tuner = kt.Hyperband(\n",
    "    build_lstm_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory='tuning',\n",
    "    project_name=f'lstm_{best_feature_type}'\n",
    ")\n",
    "\n",
    "transformer_tuner = kt.Hyperband(\n",
    "    build_transformer_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory='tuning',\n",
    "    project_name=f'transformer_{best_feature_type}'\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters\n",
    "print(\"\\nStarting CNN hyperparameter tuning...\")\n",
    "cnn_tuner.search(\n",
    "    train_ctc_ds,\n",
    "    validation_data=val_ctc_ds,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nStarting LSTM hyperparameter tuning...\")\n",
    "lstm_tuner.search(\n",
    "    train_ctc_ds,\n",
    "    validation_data=val_ctc_ds,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Transformer hyperparameter tuning...\")\n",
    "transformer_tuner.search(\n",
    "    train_ctc_ds,\n",
    "    validation_data=val_ctc_ds,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_cnn_hps = cnn_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_lstm_hps = lstm_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_transformer_hps = transformer_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Best Hyperparameters Found\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nBest CNN Hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_cnn_hps.get('learning_rate')}\")\n",
    "print(f\"Filters: {best_cnn_hps.get('filters')}\")\n",
    "print(f\"Dropout Rate: {best_cnn_hps.get('dropout_rate')}\")\n",
    "print(f\"LSTM Units: {best_cnn_hps.get('lstm_units')}\")\n",
    "\n",
    "print(\"\\nBest LSTM Hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_lstm_hps.get('learning_rate')}\")\n",
    "print(f\"LSTM Units: {best_lstm_hps.get('lstm_units')}\")\n",
    "print(f\"Dropout Rate: {best_lstm_hps.get('dropout_rate')}\")\n",
    "print(f\"Number of Layers: {best_lstm_hps.get('num_layers')}\")\n",
    "\n",
    "print(\"\\nBest Transformer Hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_transformer_hps.get('learning_rate')}\")\n",
    "print(f\"d_model: {best_transformer_hps.get('d_model')}\")\n",
    "print(f\"Number of Heads: {best_transformer_hps.get('num_heads')}\")\n",
    "print(f\"FF Dimension: {best_transformer_hps.get('ff_dim')}\")\n",
    "print(f\"Number of Layers: {best_transformer_hps.get('num_layers')}\")\n",
    "\n",
    "# Build and train best models\n",
    "print(\"\\nTraining best models with optimized hyperparameters...\")\n",
    "\n",
    "# CNN\n",
    "best_cnn_model = cnn_tuner.hypermodel.build(best_cnn_hps)\n",
    "cnn_history = best_cnn_model.fit(\n",
    "    train_ctc_ds,\n",
    "    validation_data=val_ctc_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# LSTM\n",
    "best_lstm_model = lstm_tuner.hypermodel.build(best_lstm_hps)\n",
    "lstm_history = best_lstm_model.fit(\n",
    "    train_ctc_ds,\n",
    "    validation_data=val_ctc_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Transformer\n",
    "best_transformer_model = transformer_tuner.hypermodel.build(best_transformer_hps)\n",
    "transformer_history = best_transformer_model.fit(\n",
    "    train_ctc_ds,\n",
    "    validation_data=val_ctc_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save best models\n",
    "best_cnn_model.save('models/cnn_tuned_best.h5')\n",
    "best_lstm_model.save('models/lstm_tuned_best.h5')\n",
    "best_transformer_model.save('models/transformer_tuned_best.h5')\n",
    "\n",
    "# Save training histories\n",
    "with open('cnn_tuned_history.json', 'w') as f:\n",
    "    json.dump(cnn_history.history, f, indent=2)\n",
    "\n",
    "with open('lstm_tuned_history.json', 'w') as f:\n",
    "    json.dump(lstm_history.history, f, indent=2)\n",
    "\n",
    "with open('transformer_tuned_history.json', 'w') as f:\n",
    "    json.dump(transformer_history.history, f, indent=2)\n",
    "\n",
    "# Save hyperparameters\n",
    "best_hps = {\n",
    "    'cnn': {\n",
    "        'learning_rate': best_cnn_hps.get('learning_rate'),\n",
    "        'filters': best_cnn_hps.get('filters'),\n",
    "        'dropout_rate': best_cnn_hps.get('dropout_rate'),\n",
    "        'lstm_units': best_cnn_hps.get('lstm_units')\n",
    "    },\n",
    "    'lstm': {\n",
    "        'learning_rate': best_lstm_hps.get('learning_rate'),\n",
    "        'lstm_units': best_lstm_hps.get('lstm_units'),\n",
    "        'dropout_rate': best_lstm_hps.get('dropout_rate'),\n",
    "        'num_layers': best_lstm_hps.get('num_layers')\n",
    "    },\n",
    "    'transformer': {\n",
    "        'learning_rate': best_transformer_hps.get('learning_rate'),\n",
    "        'd_model': best_transformer_hps.get('d_model'),\n",
    "        'num_heads': best_transformer_hps.get('num_heads'),\n",
    "        'ff_dim': best_transformer_hps.get('ff_dim'),\n",
    "        'num_layers': best_transformer_hps.get('num_layers')\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_hps, f, indent=2)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(cnn_history.history['val_loss'], label='CNN Tuned')\n",
    "plt.title('CNN Tuned - Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(lstm_history.history['val_loss'], label='LSTM Tuned')\n",
    "plt.title('LSTM Tuned - Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(transformer_history.history['val_loss'], label='Transformer Tuned')\n",
    "plt.title('Transformer Tuned - Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tuned_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHyperparameter tuning completed!\")\n",
    "print(\"Best models saved in 'models/' directory\")\n",
    "print(\"Hyperparameters saved in 'best_hyperparameters.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
