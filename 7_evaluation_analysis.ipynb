{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "from utils.data_loader import LibriSpeechDataLoader\n",
    "from utils.evaluation import ModelEvaluator\n",
    "from utils.model_utils import decode_predictions\n",
    "import config\n",
    "\n",
    "# Set random seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load all necessary data\n",
    "char_to_num = np.load('char_to_num.npy', allow_pickle=True).item()\n",
    "num_to_char = np.load('num_to_char.npy', allow_pickle=True).item()\n",
    "feature_info = np.load('feature_info.npy', allow_pickle=True).item()\n",
    "\n",
    "print(\"Comprehensive Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(char_to_num, num_to_char)\n",
    "\n",
    "# Load test datasets\n",
    "test_datasets = {}\n",
    "for feature_type in config.FEATURE_CONFIG['feature_types']:\n",
    "    feature_datasets = np.load(f'feature_datasets_{feature_type}.npy', allow_pickle=True).item()\n",
    "    test_datasets[feature_type] = feature_datasets['test']\n",
    "\n",
    "# Load all trained models\n",
    "models = {}\n",
    "\n",
    "# Load baseline models\n",
    "for feature_type in config.FEATURE_CONFIG['feature_types']:\n",
    "    try:\n",
    "        models[f'cnn_{feature_type}'] = keras.models.load_model(\n",
    "            f'models/cnn_{feature_type}_best.h5',\n",
    "            custom_objects={'CTCLayer': None}  # You might need to adjust this\n",
    "        )\n",
    "        models[f'lstm_{feature_type}'] = keras.models.load_model(\n",
    "            f'models/lstm_{feature_type}_best.h5',\n",
    "            custom_objects={'CTCLayer': None}\n",
    "        )\n",
    "        models[f'transformer_{feature_type}'] = keras.models.load_model(\n",
    "            f'models/transformer_{feature_type}_best.h5',\n",
    "            custom_objects={'CTCLayer': None}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load some models: {e}\")\n",
    "\n",
    "# Load tuned models\n",
    "try:\n",
    "    models['cnn_tuned'] = keras.models.load_model('models/cnn_tuned_best.h5')\n",
    "    models['lstm_tuned'] = keras.models.load_model('models/lstm_tuned_best.h5')\n",
    "    models['transformer_tuned'] = keras.models.load_model('models/transformer_tuned_best.h5')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load tuned models: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(models)} models for evaluation\")\n",
    "\n",
    "# Comprehensive evaluation function\n",
    "def evaluate_model_comprehensive(model, test_dataset, model_name, num_samples=100):\n",
    "    \"\"\"Comprehensive evaluation of a single model\"\"\"\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'test_loss': 0,\n",
    "        'wer_scores': [],\n",
    "        'cer_scores': [],\n",
    "        'predictions': [],\n",
    "        'actuals': []\n",
    "    }\n",
    "    \n",
    "    sample_count = 0\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Calculate test loss\n",
    "    for batch in test_dataset.take(10):  # Use first 10 batches for loss calculation\n",
    "        features, labels = batch\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Create CTC inputs\n",
    "        input_length = np.ones((batch_size, 1)) * features.shape[1]\n",
    "        label_length = np.ones((batch_size, 1)) * labels.shape[1]\n",
    "        dummy_labels = np.zeros(batch_size)\n",
    "        \n",
    "        loss = model.test_on_batch(\n",
    "            [features, labels, input_length, label_length],\n",
    "            dummy_labels\n",
    "        )\n",
    "        \n",
    "        total_loss += loss * batch_size\n",
    "        batch_count += batch_size\n",
    "        \n",
    "        # Calculate WER and CER for samples\n",
    "        for i in range(min(batch_size, 5)):  # 5 samples per batch\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            sample_features = features[i:i+1]\n",
    "            sample_labels = labels[i]\n",
    "            \n",
    "            # Get prediction\n",
    "            prediction = evaluator.predict_and_decode(model, sample_features, num_to_char)\n",
    "            \n",
    "            # Get actual text\n",
    "            actual_text = evaluator.numbers_to_text(sample_labels.numpy())\n",
    "            \n",
    "            # Calculate metrics\n",
    "            wer = evaluator.calculate_wer(actual_text, prediction)\n",
    "            cer = evaluator.calculate_cer(actual_text, prediction)\n",
    "            \n",
    "            results['wer_scores'].append(wer)\n",
    "            results['cer_scores'].append(cer)\n",
    "            results['predictions'].append(prediction)\n",
    "            results['actuals'].append(actual_text)\n",
    "            \n",
    "            sample_count += 1\n",
    "    \n",
    "    results['test_loss'] = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "    results['avg_wer'] = np.mean(results['wer_scores']) if results['wer_scores'] else float('inf')\n",
    "    results['avg_cer'] = np.mean(results['cer_scores']) if results['cer_scores'] else float('inf')\n",
    "    \n",
    "    print(f\"  Test Loss: {results['test_loss']:.4f}\")\n",
    "    print(f\"  Average WER: {results['avg_wer']:.4f}\")\n",
    "    print(f\"  Average CER: {results['avg_cer']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all models\n",
    "all_results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Determine feature type from model name\n",
    "    feature_type = None\n",
    "    for ft in config.FEATURE_CONFIG['feature_types']:\n",
    "        if ft in model_name:\n",
    "            feature_type = ft\n",
    "            break\n",
    "    \n",
    "    if feature_type and feature_type in test_datasets:\n",
    "        results = evaluate_model_comprehensive(model, test_datasets[feature_type], model_name)\n",
    "        all_results.append(results)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df[['model', 'test_loss', 'avg_wer', 'avg_cer']]\n",
    "results_df = results_df.sort_values('avg_wer')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Comprehensive Model Comparison\")\n",
    "print(\"=\"*50)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Test Loss Comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(results_df['model'], results_df['test_loss'], alpha=0.7)\n",
    "plt.title('Test Loss Comparison')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 2: WER Comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(results_df['model'], results_df['avg_wer'], alpha=0.7, color='orange')\n",
    "plt.title('Word Error Rate (WER) Comparison')\n",
    "plt.ylabel('WER')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 3: CER Comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(results_df['model'], results_df['avg_cer'], alpha=0.7, color='green')\n",
    "plt.title('Character Error Rate (CER) Comparison')\n",
    "plt.ylabel('CER')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 4: Model Type Performance\n",
    "plt.subplot(2, 2, 4)\n",
    "model_types = []\n",
    "wer_by_type = []\n",
    "\n",
    "for model_name in results_df['model']:\n",
    "    if 'cnn' in model_name:\n",
    "        model_types.append('CNN')\n",
    "    elif 'lstm' in model_name:\n",
    "        model_types.append('LSTM')\n",
    "    elif 'transformer' in model_name:\n",
    "        model_types.append('Transformer')\n",
    "    else:\n",
    "        model_types.append('Other')\n",
    "\n",
    "results_df['model_type'] = model_types\n",
    "type_performance = results_df.groupby('model_type')['avg_wer'].mean()\n",
    "\n",
    "plt.bar(type_performance.index, type_performance.values, alpha=0.7, color='red')\n",
    "plt.title('Average WER by Model Type')\n",
    "plt.ylabel('WER')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comprehensive_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature type analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Feature Type Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "feature_performance = {}\n",
    "for feature_type in config.FEATURE_CONFIG['feature_types']:\n",
    "    feature_models = [r for r in all_results if feature_type in r['model']]\n",
    "    if feature_models:\n",
    "        avg_wer = np.mean([r['avg_wer'] for r in feature_models])\n",
    "        feature_performance[feature_type] = avg_wer\n",
    "        print(f\"{feature_type}: Average WER = {avg_wer:.4f}\")\n",
    "\n",
    "# Best model identification\n",
    "best_model_idx = results_df['avg_wer'].idxmin()\n",
    "best_model = results_df.loc[best_model_idx]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"BEST MODEL IDENTIFIED\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Model: {best_model['model']}\")\n",
    "print(f\"Test Loss: {best_model['test_loss']:.4f}\")\n",
    "print(f\"Word Error Rate: {best_model['avg_wer']:.4f}\")\n",
    "print(f\"Character Error Rate: {best_model['avg_cer']:.4f}\")\n",
    "\n",
    "# Sample predictions from best model\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Sample Predictions from Best Model\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "best_model_name = best_model['model']\n",
    "best_model_obj = models[best_model_name]\n",
    "\n",
    "# Determine feature type for best model\n",
    "best_feature_type = None\n",
    "for ft in config.FEATURE_CONFIG['feature_types']:\n",
    "    if ft in best_model_name:\n",
    "        best_feature_type = ft\n",
    "        break\n",
    "\n",
    "if best_feature_type:\n",
    "    test_ds = test_datasets[best_feature_type]\n",
    "    \n",
    "    print(\"Sample predictions (showing first 5):\")\n",
    "    sample_count = 0\n",
    "    for features, labels in test_ds.take(3):\n",
    "        for i in range(features.shape[0]):\n",
    "            if sample_count >= 5:\n",
    "                break\n",
    "                \n",
    "            sample_features = features[i:i+1]\n",
    "            sample_labels = labels[i]\n",
    "            \n",
    "            # Get prediction\n",
    "            prediction = evaluator.predict_and_decode(best_model_obj, sample_features, num_to_char)\n",
    "            \n",
    "            # Get actual text\n",
    "            actual_text = evaluator.numbers_to_text(sample_labels.numpy())\n",
    "            \n",
    "            # Calculate metrics\n",
    "            wer = evaluator.calculate_wer(actual_text, prediction)\n",
    "            cer = evaluator.calculate_cer(actual_text, prediction)\n",
    "            \n",
    "            print(f\"\\nSample {sample_count + 1}:\")\n",
    "            print(f\"  Actual: '{actual_text}'\")\n",
    "            print(f\"  Predicted: '{prediction}'\")\n",
    "            print(f\"  WER: {wer:.4f}, CER: {cer:.4f}\")\n",
    "            \n",
    "            sample_count += 1\n",
    "\n",
    "# Save comprehensive results\n",
    "comprehensive_results = {\n",
    "    'all_results': all_results,\n",
    "    'best_model': best_model.to_dict(),\n",
    "    'feature_performance': feature_performance,\n",
    "    'summary_stats': {\n",
    "        'total_models_evaluated': len(all_results),\n",
    "        'best_wer': best_model['avg_wer'],\n",
    "        'best_cer': best_model['avg_cer'],\n",
    "        'average_wer': results_df['avg_wer'].mean(),\n",
    "        'average_cer': results_df['avg_cer'].mean()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('comprehensive_evaluation_results.json', 'w') as f:\n",
    "    json.dump(comprehensive_results, f, indent=2)\n",
    "\n",
    "# Save results DataFrame\n",
    "results_df.to_csv('model_evaluation_results.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"EVALUATION COMPLETED!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"Results saved to:\")\n",
    "print(\"- comprehensive_evaluation_results.json\")\n",
    "print(\"- model_evaluation_results.csv\")\n",
    "print(\"- comprehensive_evaluation.png\")\n",
    "print(f\"\\nBest model: {best_model['model']}\")\n",
    "print(f\"Best WER: {best_model['avg_wer']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
